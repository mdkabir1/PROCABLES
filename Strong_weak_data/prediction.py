# !/usr/bin/env python
# coding: utf-
from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
import pandas as pd
import numpy as np
import math, random, warnings, itertools, re, os, sys
from collections import Counter
from sklearn.preprocessing import scale
from gensim import corpora, models, similarities
from keras.models import load_model
import keras.backend as K
from sklearn.model_selection import KFold
from keras.layers import Input, Dense, Conv1D, Flatten, MaxPooling1D, Conv2D, MaxPooling2D, AveragePooling2D, Dropout, \
    Reshape, normalization
from keras.models import Model
from keras.layers.recurrent import LSTM
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from keras.models import model_from_json

def prediction_SW(input_data,save_model):
       # Provide the path to your FASTA file
    input_file = input_data
    model_name = save_model  # load model generated by train_CNN_model.ipynb
    def read_nucleotide_sequences(file):
        if os.path.exists(file) == False:
            print('Error: file %s does not exist.' % file)
            sys.exit(1)
        with open(file) as f:
            records = f.read()
        if re.search('>', records) == None:
            print('Error: the input file %s seems not in FASTA format!' % file)
            sys.exit(1)
        records = records.split('>')[1:]
        fasta_sequences = []
        for fasta in records:
            array = fasta.split('\n')
            header, sequence = array[0].split()[0], re.sub('[^ACGTU-]', '-', ''.join(array[1:]).upper())
            header_array = header.split('|')
            name = header_array[0]
            label = header_array[1] if len(header_array) >= 2 else '0'
            label_train = header_array[2] if len(header_array) >= 3 else 'training'
            sequence = re.sub('U', 'T', sequence)
            fasta_sequences.append([name, sequence, label, label_train])
        return fasta_sequences

    def check_fasta_with_equal_length(fastas):
        status = True
        lenList = set()
        for i in fastas:
            lenList.add(len(i[1]))
        if len(lenList) == 1:
            return True
        else:
            return False

    def get_min_sequence_length(fastas):
        minLen = 10000
        for i in fastas:
            if minLen > len(i[1]):
                minLen = len(i[1])
        return minLen

    def get_min_sequence_length_1(fastas):
        minLen = 10000
        for i in fastas:
            if minLen > len(re.sub('-', '', i[1])):
                minLen = len(re.sub('-', '', i[1]))
        return minLen
    def TNC(fastas, **kw):
        AA = 'ACGT'
        encodings = []
        triPeptides = [aa1 + aa2 + aa3 for aa1 in AA for aa2 in AA for aa3 in AA]
        #header = ['#', 'label'] + triPeptides
        #encodings.append(header)

        AADict = {}
        for i in range(len(AA)):
            AADict[AA[i]] = i

        for i in fastas:
            name, sequence = i[0], re.sub('-', '', i[1])
            code = []
            tmpCode = [0] * 64
            for j in range(len(sequence) - 3 + 1):
                tmpCode[AADict[sequence[j]] * 16 + AADict[sequence[j+1]]*4 + AADict[sequence[j+2]]] = tmpCode[AADict[sequence[j]] * 16 + AADict[sequence[j+1]]*4 + AADict[sequence[j+2]]] +1
            if sum(tmpCode) != 0:
                tmpCode = [i/sum(tmpCode) for i in tmpCode]
            code = code + tmpCode
            encodings.append(code)
        return np.array(encodings,dtype=float)
    def DNA2Sentence(dna, K):

        sentence = ""
        length = len(dna)

        for i in range(length - K + 1):
            sentence += dna[i: i + K] + " "

        #delete extra space
        sentence = sentence[0 : len(sentence) - 1]
        return sentence

    def Get_Unsupervised(fname,gname,kmer):
        f = open(fname,'r')
        g = open(gname,'w')
        k = kmer
        for i in f:
            if '>' not in i:
                i = i.strip('\n').upper()
                line = DNA2Sentence(i,k)
                g.write(line+'\n')
        f.close()
        g.close()
    #combine two corpus and generate final corpus
    with open('Train_2Unsuper','ab') as f:
            f.write(open('Train_Pos2Unsuper','rb').read())
            #f.write(open('Train_Neg2Unsuper','rb').read())
    #get model
    def getWord_model(word,num_features,min_count):
        word_model = ""
        if not os.path.isfile("Train_model1"):
            sentence = LineSentence("Train_2Unsuper",max_sentence_length = 15000)
            print ("Start Training Word2Vec model...")
            # Set values for various parameters
            num_features = int(num_features)	  # Word vector dimensionality
            min_word_count = int(min_count)	  # Minimum word count
            num_workers = 20		 # Number of threads to run in parallel
            context = 20			# Context window size
            downsampling = 1e-3	 # Downsample setting for frequent words

            # Initialize and train the model
            print ("Training Word2Vec model...")
            word_model = Word2Vec(sentence, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling, seed=1,iter = 50)
            word_model.init_sims(replace=False)
            word_model.save("Train_model1")
            #print word_model.most_similar("CATAGT")
        else:
            print ("Loading Word2Vec model...")
            word_model = Word2Vec.load("Train_model1")
            #word_model.init_sims(replace=True)
        return word_model

    getWord_model(2,200,1)

    def combine(Posfile, Combfile):
        f1 = open(Posfile)
        #f2 = open(Negfile)
        g = open(Combfile,'w')
        g.write('lable\tseq\n')
        for i in f1:
            if '>'not in i:
                g.write('1\t'+i)
        #for i in f2:
        #    if '>'not in i:
        #        g.write('0\t'+i)
        #f1.close()
        #f2.close()
        g.close()
    #'negative_train.fasta',
    #obtain feature file with .npy format
    def getDNA_split(DNAdata,word):
        DNAlist1 = []
        #DNAlist2 = []
        counter = 0
        for DNA in DNAdata["seq"]:
            #if counter % 100 == 0:
                #print ("DNA %d of %d\r" % (counter, 2*len(DNAdata)))
                #sys.stdout.flush()

            DNA = str(DNA).upper()
            DNAlist1.append(DNA2Sentence(DNA,word).split(" "))#[['ACG', 'CGT', 'GTC'],['ACG', 'CGT', 'GTC'],['ACG', 'CGT', 'GTC']]

            counter += 1
        return DNAlist1

    def getAvgFeatureVecs(DNAdata1,model,num_features):
        counter = 0
        DNAFeatureVecs = np.zeros((len(DNAdata1),num_features), dtype="float32")
        for DNA in DNAdata1:
            if counter % 1000 == 0:
                print ("DNA %d of %d\r" % (counter, len(DNAdata1)))
                sys.stdout.flush()

            DNAFeatureVecs[counter][0:num_features] = np.mean(model[DNA],axis = 0)
            counter += 1
        print()
        counter = 0
        return DNAFeatureVecs

    def npyToTXT(npyfile, svmfile, pos_num):
        dataDataVecs = np.load(npyfile)
        g = open(svmfile, 'w')
        print(len(dataDataVecs))
        # print(dataDataVecs[0])

        m = 0
        for i in range(len(dataDataVecs)):
            line = ''
            for j in range(len(dataDataVecs[0])):
                if j == len(dataDataVecs[0]) - 1:
                    line += str(j + 1) + ':' + str(dataDataVecs[i][j]) + '\n'
                else:
                    line += str(j + 1) + ':' + str(dataDataVecs[i][j]) + '\t'
            m += 1
            if m < (pos_num + 1):
                g.write('1\t' + line)
            else:
                g.write('0\t' + line)

    def TXTtoCSV(svmfile, csvfile):
        f = open(svmfile, 'r')
        g = open(csvfile, 'w')
        lines = f.readlines()
        legth = len(lines[0].split('	')) - 1
        # print(legth)
        classline = 'class'
        for i in range(legth):
            classline += ',%d' % (i + 1)
        g.write(classline + '\n')

        for line in lines:
            line = line.strip('\n').split('	')
            g.write(line[0] + ',')

            legth2 = len(line[1:])
            m = 0
            for j in line[1:]:
                if m == legth2 - 1:
                    j = j.split(':')[-1]
                    g.write(j)
                    m += 1
                else:
                    j = j.split(':')[-1]
                    g.write(j + ',')
                    m += 1
            g.write('\n')

        f.close()
        g.close()
    def TriNcleotideComposition(sequence, base):
        trincleotides = [nn1 + nn2 + nn3 for nn1 in base for nn2 in base for nn3 in base]
        tnc_dict = {}
        for triN in trincleotides:
            tnc_dict[triN] = 0
        for i in range(len(sequence) - 2):
            tnc_dict[sequence[i:i + 3]] += 1
        for key in tnc_dict:
           tnc_dict[key] /= (len(sequence) - 2)
        return tnc_dict

    def PseEIIP(fastas, **kw):
        for i in fastas:
            if re.search('[^ACGT-]', i[1]):
                print('Error: illegal character included in the fasta sequences, only the "ACGT-" are allowed by this PseEIIP scheme.')
                return 0

        base = 'ACGT'

        EIIP_dict = {
            'A': 0.1260,
            'C': 0.1340,
            'G': 0.0806,
            'T': 0.1335,
        }

        trincleotides = [nn1 + nn2 + nn3 for nn1 in base for nn2 in base for nn3 in base]
        EIIPxyz = {}
        for triN in trincleotides:
            EIIPxyz[triN] = EIIP_dict[triN[0]] + EIIP_dict[triN[1]] + EIIP_dict[triN[2]]

        encodings = []
        #header = ['#', 'label'] + trincleotides
        #encodings.append(header)

        for i in fastas:
            name, sequence = i[0], re.sub('-', '', i[1])
            code = []
            trincleotide_frequency = TriNcleotideComposition(sequence, base)
            code = code + [EIIPxyz[triN] * trincleotide_frequency[triN] for triN in trincleotides]
            encodings.append(code)
        return np.array(encodings,dtype=float)
 # Define a function to count sequences in a FASTA file
    def count_sequences(fasta_file):
        with open(fasta_file, 'r') as f:
            count = 0
            for line in f:
                if line.startswith('>'):
                    count += 1
        return count

    # Call the function to count sequences
    sequence_count = count_sequences(input_file)
    m=(sequence_count//2)+5
    n=(sequence_count//2)+6
    seq = []
    m6a_2614_sequence = input_file

    RNA_code = 'ACGT'
    k = 1
    fh = open(m6a_2614_sequence)
    for line in fh:
        if line.startswith('>'):
            continue
        else:
            seq.append(line.replace('\n', ''))
    fh.close()
    def make_kmer_list(k, alphabet):
        try:
            return ["".join(e) for e in itertools.product(alphabet, repeat=k)]
        except TypeError:
            print("TypeError: k must be an inter and larger than 0, alphabet must be a string.")
            raise TypeError
        except ValueError:
            print("TypeError: k must be an inter and larger than 0")
            raise ValueError
    positive_seq = seq[:m]
    negative_seq = seq[n:]
    for interval in range(1, k + 1):
        final_seq_value1 = [[0 for ii in range(len(seq[0]) - interval)] for jj in range(len(seq))]
        code_values = make_kmer_list(interval, RNA_code)
        code_len = len(code_values)

        positive_seq_value = [[0 for jj in range(len(seq[0]) - 1)] for ii in range(code_len)]
        negative_seq_value = [[0 for jj in range(len(seq[0]) - 1)] for ii in range(code_len)]
        for i, line_value in enumerate(positive_seq):
            for j, code_value in enumerate(line_value):
                if j <= len(line_value) - interval - 1:
                    for p, c_value in enumerate(code_values):
                        if c_value == line_value[j:j + interval]:
                            positive_seq_value[p][j] += 1
        positive_seq_value = np.matrix(positive_seq_value) * 1.0 / (len(seq) / 2)
        for i, line_value in enumerate(negative_seq):
            for j, code_value in enumerate(line_value):
                if j <= len(line_value) - interval - 1:
                    for p, c_value in enumerate(code_values):
                        if c_value == line_value[j:j + interval]:
                            negative_seq_value[p][j] += 1
        negative_seq_value = np.matrix(negative_seq_value) * 1.0 / (len(seq) / 2)
        for i, line_value in enumerate(seq):
            for j, code_value in enumerate(line_value):
                if j <= len(line_value) - interval - 1:
                    for p, c_value in enumerate(code_values):
                        if c_value == line_value[j:j + interval]:
                            final_seq_value1[i][j] = positive_seq_value[p, j] - negative_seq_value[p, j]

    seq = []
    k = 2
    fh = open(m6a_2614_sequence)
    for line in fh:
        if line.startswith('>'):
            continue
        else:
            seq.append(line.replace('\n', ''))
    fh.close()

    def make_kmer_list(k, alphabet):
        try:
            return ["".join(e) for e in itertools.product(alphabet, repeat=k)]
        except TypeError:
            print("TypeError: k must be an inter and larger than 0, alphabet must be a string.")
            raise TypeError
        except ValueError:
            print("TypeError: k must be an inter and larger than 0")
            raise ValueError
    positive_seq = seq[:m]
    negative_seq = seq[n:]
    for interval in range(1, k + 1):
        final_seq_value2 = [[0 for ii in range(len(seq[0]) - interval)] for jj in range(len(seq))]
        code_values = make_kmer_list(interval, RNA_code)
        code_len = len(code_values)

        positive_seq_value = [[0 for jj in range(len(seq[0]) - 1)] for ii in range(code_len)]
        negative_seq_value = [[0 for jj in range(len(seq[0]) - 1)] for ii in range(code_len)]
        for i, line_value in enumerate(positive_seq):
            for j, code_value in enumerate(line_value):
                if j <= len(line_value) - interval - 1:
                    for p, c_value in enumerate(code_values):
                        if c_value == line_value[j:j + interval]:
                            positive_seq_value[p][j] += 1

        positive_seq_value = np.matrix(positive_seq_value) * 1.0 / (len(seq) / 2)
        for i, line_value in enumerate(negative_seq):
            for j, code_value in enumerate(line_value):
                if j <= len(line_value) - interval - 1:
                    for p, c_value in enumerate(code_values):
                        if c_value == line_value[j:j + interval]:
                            negative_seq_value[p][j] += 1
        negative_seq_value = np.matrix(negative_seq_value) * 1.0 / (len(seq) / 2)
        for i, line_value in enumerate(seq):
            for j, code_value in enumerate(line_value):
                if j <= len(line_value) - interval - 1:
                    for p, c_value in enumerate(code_values):
                        if c_value == line_value[j:j + interval]:
                            final_seq_value2[i][j] = positive_seq_value[p, j] - negative_seq_value[p, j]

    #pd.DataFrame(final_seq_value).to_csv('%d_interval_SW_train_PSTNP.csv' % interval, header=None, index=False)

    combine(input_file,'Train_Comb.fasta')#'pos.fasta' contains positive samples with fasta format; 'neg.fasta' contains negative samples with fasta format; 'all.fasta' is a combination file of positive and negative samples.
    Get_Unsupervised(input_file,'Train_2Unsuper',2)#postive samples contained in 'pos.fasta';'pos2Un' is outputFile of corpus;'2' is the size of Kmer
    data = pd.read_csv('Train_Comb.fasta',sep = "\t",error_bad_lines=False)
    datawords1 = getDNA_split(data,2)#size of kmer
    word_model = Word2Vec.load("Train_model1")#'model_2' is above generated in 'get model'
    dataDataVecs = getAvgFeatureVecs(datawords1,word_model,200)
    np.save("222_vecs.npy",dataDataVecs) #outputFile
    npyToTXT("222_vecs.npy", '222_vecs.txt', 222)  # positive number
    TXTtoCSV('222_vecs.txt', '222_vecs.csv')
    ###########################################################
    kw = {'order': 'ACGT'}
    fastas = read_nucleotide_sequences(input_file)
    PseEIIP_feat = PseEIIP(fastas, **kw)
    TNC_feat=TNC(fastas, **kw)
    [s, f]=np.shape(TNC_feat)
    data=pd.read_csv('222_vecs.csv')
    data1=np.array(data)
    w2vec=data1[:,1:]
    # w2vec1=pd.read_csv('Train_SW.csv',header=None)
    # df=np.array(w2vec1)
    # w2vec=df[:s,:]
    PSTN_K1=np.array(final_seq_value1)
    PSTN_K2=np.array(final_seq_value2)
    featur=np.hstack((TNC_feat,PseEIIP_feat,w2vec,PSTN_K1,PSTN_K2))
    data_feature=scale(featur)
    X = np.expand_dims(data_feature, 2)
    model_back = load_model(model_name)  #
    threshold = 0.50
    maxprobability = model_back.predict(X)
    prediction = model_back.predict(X)
    arr = np.array(prediction)
    # Format the elements of the ndarray with two decimal places
    out_prediction = np.array2string(arr, formatter={'all': lambda x: f'{x:.3f}'})
    return out_prediction,maxprobability
# print('T type of phosphorylation sites predicted prob as follows.')
# for i in range(m):
#     if maxprobability[i] >= threshold:
#         print('Antibiotic')
#         print('-------------------------------')
#     else:
#         print('Non_Antibiotic')
#         print('-------------------------------')
# np.savetxt('Predicted_Antibiotic.csv', np.asarray(prediction), delimiter=',', fmt='%f')
# np.set_printoptions(threshold=np.inf)
# print(maxprobability)  # print maxprobability
